{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n        print(dirname)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-09T12:10:18.125617Z","iopub.execute_input":"2026-01-09T12:10:18.125996Z","iopub.status.idle":"2026-01-09T12:10:30.943837Z","shell.execute_reply.started":"2026-01-09T12:10:18.125961Z","shell.execute_reply":"2026-01-09T12:10:30.942575Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Iâ€™m Something of a Painter Myself  \n### Mini-Project: Generative Adversarial Networks for Artistic Style Generation\n\nThis mini-project is focused on building and training deep generative learning models. The goal is to gain hands-on experience with Generative Adversarial Networks (GANs) by applying them to an image generation task inspired by artistic style transfer. The project uses a Kaggle â€œGetting Startedâ€ competition as a practical framework for model development, evaluation, and comparison.\n\n---\n\n## Problem Description\n\nThe objective of this project is to build a generative model capable of producing images that resemble the artistic style of Claude Monet. Specifically, the task is to train a Generative Adversarial Network (GAN) to generate novel images that capture characteristic features of Monetâ€™s paintings, such as color palettes, textures, and brush-stroke patterns.\n\nGANs consist of two competing neural networks: a generator, which attempts to create realistic images, and a discriminator, which learns to distinguish between real images and generated ones. Through this adversarial training process, the generator progressively improves its ability to produce high-quality, realistic images.  \n\nThis problem is relevant because it demonstrates how modern deep learning models can learn complex visual distributions and creative styles, highlighting the intersection between computer vision, unsupervised learning, and computational creativity.\n","metadata":{}},{"cell_type":"markdown","source":"## Data Description\n\nThe dataset used in this project consists of image data provided by the Kaggle *â€œIâ€™m Something of a Painter Myselfâ€* competition. The data is designed for training generative models to learn and reproduce an artistic style.\n\n- **Data type**:  \n  - Monet paintings (target domain), representing the artistic style to be learned.  \n  - Photographs (source domain), which can be used as input images when applying style transfer approaches such as CycleGAN.\n\n- **Image format**:  \n  - Images are RGB with shape **256 Ã— 256 Ã— 3** after preprocessing/resizing (as required for model training and submission).\n\n- **Data size (as observed in this notebook environment)**:  \n  - **Monet JPG** folder: **300** images  \n  - **Photo JPG** folder: **7,038** images\n\n- **Data structure**:  \n  - Images are available both as **JPG files** (`monet_jpg/`, `photo_jpg/`) and as **TFRecords** (`monet_tfrec/`, `photo_tfrec/`) for more efficient TensorFlow input pipelines.  \n  - No labels are provided, since this is an **unsupervised** image generation / style learning task.\n\nOverall, the dataset is suitable for training GAN-based models to learn the visual distribution of Monet-style paintings and generate new images consistent with that style.\n","metadata":{}},{"cell_type":"code","source":"# =========================\n# Data Loading + EDA (CPU-only)\n# =========================\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"   # 0=all, 1=filter INFO, 2=filter WARNING, 3=filter ERROR\n\nimport glob, random\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport tensorflow as tf\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n\n# -------------------------\n# Paths (given)\n# -------------------------\nROOT = Path(\"/kaggle/input/gan-getting-started\")\nMONET_JPG_DIR   = ROOT / \"monet_jpg\"\nPHOTO_JPG_DIR   = ROOT / \"photo_jpg\"\nMONET_TFREC_DIR = ROOT / \"monet_tfrec\"\nPHOTO_TFREC_DIR = ROOT / \"photo_tfrec\"\n\nprint(\"Root:\", ROOT)\nprint(\"Monet JPG:\", MONET_JPG_DIR)\nprint(\"Photo JPG:\", PHOTO_JPG_DIR)\nprint(\"Monet TFRecords:\", MONET_TFREC_DIR)\nprint(\"Photo TFRecords:\", PHOTO_TFREC_DIR)\n\n# -------------------------\n# Helpers\n# -------------------------\ndef list_files(dir_path: Path, exts=(\".jpg\", \".jpeg\", \".png\")):\n    files = []\n    for ext in exts:\n        files.extend(sorted(glob.glob(str(dir_path / f\"*{ext}\"))))\n    return files\n\ndef show_grid_from_paths(paths, rows=2, cols=3, title=None):\n    \"\"\"Show a small fixed grid (default 3x2) to avoid excessive visualization.\"\"\"\n    paths = list(paths)\n    n = rows * cols\n    if len(paths) == 0:\n        print(f\"[WARN] No images to display for: {title}\")\n        return\n\n    sample = random.sample(paths, k=min(n, len(paths)))\n    plt.figure(figsize=(cols * 4, rows * 4))\n    if title:\n        plt.suptitle(title, fontsize=14)\n\n    for i, p in enumerate(sample, start=1):\n        img = Image.open(p).convert(\"RGB\")\n        w, h = img.size\n        plt.subplot(rows, cols, i)\n        plt.imshow(img)\n        plt.axis(\"off\")\n        plt.title(f\"{w}x{h}\", fontsize=10)\n\n    plt.tight_layout()\n    plt.show()\n\ndef check_all_sizes(paths, expected_w=256, expected_h=256, label=\"\"):\n    \"\"\"\n    Verify that ALL images match expected size.\n    Reports width check and height check separately + up to 10 mismatches.\n    \"\"\"\n    paths = list(paths)\n    if len(paths) == 0:\n        print(f\"[WARN] No images found for {label}\")\n        return\n\n    width_ok = True\n    height_ok = True\n    mismatches = []\n\n    for p in paths:\n        with Image.open(p) as img:\n            w, h = img.size\n        if w != expected_w:\n            width_ok = False\n        if h != expected_h:\n            height_ok = False\n        if (w != expected_w) or (h != expected_h):\n            mismatches.append((p, w, h))\n\n    print(f\"\\n[{label}] Total images: {len(paths)}\")\n    print(f\"[{label}] All widths == {expected_w}: {width_ok}\")\n    print(f\"[{label}] All heights == {expected_h}: {height_ok}\")\n\n    if mismatches:\n        print(f\"[{label}] Mismatches found: {len(mismatches)} (showing up to 10)\")\n        for p, w, h in mismatches[:10]:\n            print(f\"  - {p} -> {w}x{h}\")\n    else:\n        print(f\"[{label}] No size mismatches found.\")\n\n# -------------------------\n# Load JPG file lists\n# -------------------------\nmonet_jpg_paths = list_files(MONET_JPG_DIR, exts=(\".jpg\", \".jpeg\"))\nphoto_jpg_paths = list_files(PHOTO_JPG_DIR, exts=(\".jpg\", \".jpeg\"))\n\nprint(f\"Monet JPG count: {len(monet_jpg_paths)}\")\nprint(f\"Photo JPG count: {len(photo_jpg_paths)}\")\n\n# -------------------------\n# TFRecords discovery (informational)\n# -------------------------\nmonet_tfrec_files = sorted(glob.glob(str(MONET_TFREC_DIR / \"*.tfrec\"))) + sorted(glob.glob(str(MONET_TFREC_DIR / \"*.tfrecord*\")))\nphoto_tfrec_files = sorted(glob.glob(str(PHOTO_TFREC_DIR / \"*.tfrec\"))) + sorted(glob.glob(str(PHOTO_TFREC_DIR / \"*.tfrecord*\")))\n\nprint(f\"Monet TFRecord files: {len(monet_tfrec_files)}\")\nprint(f\"Photo TFRecord files: {len(photo_tfrec_files)}\")\n\n# -------------------------\n# EDA: show few sample images (3x2 each)\n# -------------------------\nshow_grid_from_paths(monet_jpg_paths, rows=2, cols=3, title=\"Monet JPG Samples (3x2)\")\nshow_grid_from_paths(photo_jpg_paths, rows=2, cols=3, title=\"Photo JPG Samples (3x2)\")\n\n# -------------------------\n# EDA: size checks only \n# -------------------------\ncheck_all_sizes(monet_jpg_paths, expected_w=256, expected_h=256, label=\"Monet JPG\")\ncheck_all_sizes(photo_jpg_paths, expected_w=256, expected_h=256, label=\"Photo JPG\")\n\n# -------------------------\n# Preprocessing functions \n# -------------------------\nIMG_SIZE = 256\n\ndef decode_and_resize_jpg(path, img_size=IMG_SIZE):\n    img_bytes = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img_bytes, channels=3)\n    img = tf.image.resize(img, [img_size, img_size], method=\"bilinear\")\n    img = tf.cast(img, tf.float32)\n    return img\n\ndef normalize_01(img):\n    return img / 255.0\n\ndef normalize_m11(img):\n    return (img / 127.5) - 1.0\n\n# -------------------------\n# Quick check: dtype/range after preprocessing \n# -------------------------\ndef inspect_preprocessing(paths, n=4, label=\"\"):\n    if len(paths) == 0:\n        return\n    sample = random.sample(list(paths), k=min(n, len(paths)))\n    imgs = [decode_and_resize_jpg(p) for p in sample]\n    batch = tf.stack(imgs, axis=0)  # [B, H, W, C]\n\n    batch_01 = normalize_01(batch)\n    batch_m11 = normalize_m11(batch)\n\n    print(f\"\\n[{label}] Raw batch:\", batch.shape, batch.dtype,\n          \"min/max:\", float(tf.reduce_min(batch)), float(tf.reduce_max(batch)))\n    print(f\"[{label}] 0..1 batch:\", batch_01.shape, batch_01.dtype,\n          \"min/max:\", float(tf.reduce_min(batch_01)), float(tf.reduce_max(batch_01)))\n    print(f\"[{label}] -1..1 batch:\", batch_m11.shape, batch_m11.dtype,\n          \"min/max:\", float(tf.reduce_min(batch_m11)), float(tf.reduce_max(batch_m11)))\n\ninspect_preprocessing(monet_jpg_paths, n=4, label=\"Monet\")\ninspect_preprocessing(photo_jpg_paths, n=4, label=\"Photo\")\n\n# -------------------------\n# Build tf.data pipelines (JPG) for training (CPU)\n# -------------------------\nAUTOTUNE = tf.data.AUTOTUNE\n\ndef make_jpg_dataset(paths, batch_size=16, shuffle=True, repeat=True, norm=\"m11\"):\n    ds = tf.data.Dataset.from_tensor_slices(paths)\n    if shuffle:\n        ds = ds.shuffle(buffer_size=min(len(paths), 2048), reshuffle_each_iteration=True)\n    ds = ds.map(lambda p: decode_and_resize_jpg(p, IMG_SIZE), num_parallel_calls=AUTOTUNE)\n    if norm == \"m11\":\n        ds = ds.map(normalize_m11, num_parallel_calls=AUTOTUNE)\n    elif norm == \"01\":\n        ds = ds.map(normalize_01, num_parallel_calls=AUTOTUNE)\n    if repeat:\n        ds = ds.repeat()\n    ds = ds.batch(batch_size, drop_remainder=True).prefetch(AUTOTUNE)\n    return ds\n\n# Example datasets (for later training)\nmonet_ds = make_jpg_dataset(monet_jpg_paths, batch_size=16, shuffle=True, repeat=True, norm=\"m11\")\nphoto_ds = make_jpg_dataset(photo_jpg_paths, batch_size=16, shuffle=True, repeat=True, norm=\"m11\")\n\n# Peek one batch\nm_batch = next(iter(monet_ds.take(1)))\np_batch = next(iter(photo_ds.take(1)))\nprint(\"\\nMonet batch:\", m_batch.shape, m_batch.dtype, \"min/max:\", float(tf.reduce_min(m_batch)), float(tf.reduce_max(m_batch)))\nprint(\"Photo batch:\", p_batch.shape, p_batch.dtype, \"min/max:\", float(tf.reduce_min(p_batch)), float(tf.reduce_max(p_batch)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T12:10:35.150499Z","iopub.execute_input":"2026-01-09T12:10:35.150993Z","iopub.status.idle":"2026-01-09T12:11:45.438773Z","shell.execute_reply.started":"2026-01-09T12:10:35.150961Z","shell.execute_reply":"2026-01-09T12:11:45.437718Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Choice and Motivation\n\nFor this project, a **Generative Adversarial Network (GAN)** architecture is required to generate images in the style of Claude Monet. Among the available GAN-based approaches, **CycleGAN** is a natural and well-established choice for this task.\n\nCycleGAN is designed for **unpaired image-to-image translation**, meaning it can learn a mapping between two visual domains without requiring one-to-one correspondence between images. In this case, the two domains are:\n- photographs (source domain)\n- Monet-style paintings (target domain)\n\nThis property is particularly important because the dataset does not provide paired photoâ€“painting examples. CycleGAN addresses this by using two generators and two discriminators, along with a **cycle-consistency constraint**, which encourages the translated images to preserve the underlying content of the original images while adopting the target style.\n\nThe use of CycleGAN allows the model to:\n- learn stylistic features such as color distributions and texture patterns from Monet paintings,\n- preserve structural content from photographs,\n- operate in a fully unsupervised setting, consistent with the dataset characteristics.\n\nGiven these properties, CycleGAN provides a robust and conceptually appropriate baseline for learning Monet-style image generation in this mini-project.\n","metadata":{}},{"cell_type":"code","source":"# =========================\n# CycleGAN - Model Architecture (tf.keras) \n# Generators: ResNet-style with Conv(stride=2) downsampling + Residual blocks + UpSampling\n# Discriminators: PatchGAN producing logits (no sigmoid) for BCEWithLogits\n# =========================\n\nimport tensorflow as tf\n\n# -------------------------\n# Building Blocks (merged conv block)\n# -------------------------\ndef conv_norm_act(\n    x,\n    filters,\n    kernel_size=3,\n    strides=1,\n    padding=\"same\",\n    use_norm=True,\n    act=\"relu\",          # \"relu\" or \"lrelu\"\n    lrelu_alpha=0.2,\n    name=None,\n):\n    x = tf.keras.layers.Conv2D(\n        filters,\n        kernel_size,\n        strides=strides,\n        padding=padding,\n        use_bias=not use_norm,\n        kernel_initializer=\"he_normal\",\n        name=None if name is None else f\"{name}_conv\",\n    )(x)\n\n    if use_norm:\n        x = tf.keras.layers.BatchNormalization(\n            name=None if name is None else f\"{name}_bn\"\n        )(x)\n\n    if act == \"relu\":\n        x = tf.keras.layers.ReLU(name=None if name is None else f\"{name}_relu\")(x)\n    elif act == \"lrelu\":\n        x = tf.keras.layers.LeakyReLU(lrelu_alpha, name=None if name is None else f\"{name}_lrelu\")(x)\n    else:\n        raise ValueError(\"act must be 'relu' or 'lrelu'\")\n\n    return x\n\ndef residual_block(x, filters, name=None):\n    \"\"\"\n    Minimal ResNet block for generator:\n    x -> Conv -> BN -> ReLU -> Conv -> BN -> Add(skip)\n    \"\"\"\n    skip = x\n\n    y = tf.keras.layers.Conv2D(\n        filters, 3, strides=1, padding=\"same\", use_bias=False,\n        kernel_initializer=\"he_normal\",\n        name=None if name is None else f\"{name}_c1\",\n    )(x)\n    y = tf.keras.layers.BatchNormalization(name=None if name is None else f\"{name}_bn1\")(y)\n    y = tf.keras.layers.ReLU(name=None if name is None else f\"{name}_relu\")(y)\n\n    y = tf.keras.layers.Conv2D(\n        filters, 3, strides=1, padding=\"same\", use_bias=False,\n        kernel_initializer=\"he_normal\",\n        name=None if name is None else f\"{name}_c2\",\n    )(y)\n    y = tf.keras.layers.BatchNormalization(name=None if name is None else f\"{name}_bn2\")(y)\n\n    out = tf.keras.layers.Add(name=None if name is None else f\"{name}_add\")([skip, y])\n    return out\n\n# -------------------------\n# Generator (ResNet-style)\n# -------------------------\ndef build_generator(name=\"generator\", img_size=256, base_filters=64, n_res_blocks=6):\n    \"\"\"\n    Input:  (img_size, img_size, 3) normalized to [-1, 1]\n    Output: (img_size, img_size, 3) with tanh in [-1, 1]\n    \"\"\"\n    inp = tf.keras.layers.Input(shape=(img_size, img_size, 3), name=f\"{name}_input\")\n\n    # Initial conv\n    x = conv_norm_act(inp, base_filters, kernel_size=7, strides=1, padding=\"same\",\n                      use_norm=True, act=\"relu\", name=f\"{name}_init\")\n\n    # Downsampling: 256 -> 128 -> 64\n    x = conv_norm_act(x, base_filters * 2, kernel_size=3, strides=2, padding=\"same\",\n                      use_norm=True, act=\"relu\", name=f\"{name}_down1\")\n    x = conv_norm_act(x, base_filters * 4, kernel_size=3, strides=2, padding=\"same\",\n                      use_norm=True, act=\"relu\", name=f\"{name}_down2\")\n\n    # Residual blocks at 64x64\n    for i in range(n_res_blocks):\n        x = residual_block(x, base_filters * 4, name=f\"{name}_res{i+1}\")\n\n    # Upsampling: 64 -> 128 -> 256 (nearest + conv)\n    x = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation=\"nearest\", name=f\"{name}_up1\")(x)\n    x = conv_norm_act(x, base_filters * 2, kernel_size=3, strides=1, padding=\"same\",\n                      use_norm=True, act=\"relu\", name=f\"{name}_up1_conv\")\n\n    x = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation=\"nearest\", name=f\"{name}_up2\")(x)\n    x = conv_norm_act(x, base_filters, kernel_size=3, strides=1, padding=\"same\",\n                      use_norm=True, act=\"relu\", name=f\"{name}_up2_conv\")\n\n    # Output conv -> tanh\n    out = tf.keras.layers.Conv2D(\n        3, 7, strides=1, padding=\"same\",\n        kernel_initializer=\"glorot_uniform\",\n        name=f\"{name}_out_conv\",\n    )(x)\n    out = tf.keras.layers.Activation(\"tanh\", name=f\"{name}_tanh\")(out)\n\n    return tf.keras.Model(inp, out, name=name)\n\n# -------------------------\n# PatchGAN Discriminator (logits)\n# -------------------------\ndef build_discriminator(name=\"discriminator\", img_size=256, base_filters=64):\n    \"\"\"\n    PatchGAN discriminator.\n    Input:  (img_size, img_size, 3) in [-1, 1]\n    Output: (H', W', 1) logits map (no sigmoid) for BCEWithLogits\n    \"\"\"\n    inp = tf.keras.layers.Input(shape=(img_size, img_size, 3), name=f\"{name}_input\")\n\n    # First layer usually without norm\n    x = conv_norm_act(inp, base_filters, kernel_size=4, strides=2, padding=\"same\",\n                      use_norm=False, act=\"lrelu\", name=f\"{name}_c1\")\n    x = conv_norm_act(x, base_filters * 2, kernel_size=4, strides=2, padding=\"same\",\n                      use_norm=True, act=\"lrelu\", name=f\"{name}_c2\")\n\n    x = conv_norm_act(x, base_filters * 4, kernel_size=4, strides=2, padding=\"same\",\n                      use_norm=True, act=\"lrelu\", name=f\"{name}_c3\")\n    x = conv_norm_act(x, base_filters * 8, kernel_size=4, strides=1, padding=\"same\",\n                      use_norm=True, act=\"lrelu\", name=f\"{name}_c4\")\n\n    # Final conv to 1 channel logits map (no activation)\n    out = tf.keras.layers.Conv2D(\n        1, 4, strides=1, padding=\"same\",\n        kernel_initializer=\"he_normal\",\n        name=f\"{name}_out_logits\",\n    )(x)\n\n    return tf.keras.Model(inp, out, name=name)\n\n# -------------------------\n# Instantiate CycleGAN components\n# -------------------------\nIMG_SIZE = 256\nBASE_FILTERS_G = 16\nBASE_FILTERS_D = 16\nN_RES_BLOCKS = 4  # from 3 to 6\n\nG_photo2monet = build_generator(\"G_photo2monet\", img_size=IMG_SIZE, base_filters=BASE_FILTERS_G, n_res_blocks=N_RES_BLOCKS)\nF_monet2photo = build_generator(\"F_monet2photo\", img_size=IMG_SIZE, base_filters=BASE_FILTERS_G, n_res_blocks=N_RES_BLOCKS)\n\nD_monet = build_discriminator(\"D_monet\", img_size=IMG_SIZE, base_filters=BASE_FILTERS_D)\nD_photo = build_discriminator(\"D_photo\", img_size=IMG_SIZE, base_filters=BASE_FILTERS_D)\n\n# Print summaries (optional)\nG_photo2monet.summary()\nD_monet.summary()\n\n# Quick shape check\nx = tf.random.uniform([1, IMG_SIZE, IMG_SIZE, 3], minval=-1.0, maxval=1.0)\ny = G_photo2monet(x)\nd = D_monet(y)\nprint(\"Generator output:\", y.shape, \"range approx:\", float(tf.reduce_min(y)), float(tf.reduce_max(y)))\nprint(\"Discriminator output (logits map):\", d.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T12:12:17.279097Z","iopub.execute_input":"2026-01-09T12:12:17.279455Z","iopub.status.idle":"2026-01-09T12:12:18.377070Z","shell.execute_reply.started":"2026-01-09T12:12:17.279427Z","shell.execute_reply":"2026-01-09T12:12:18.376003Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loss Functions and Training Strategy\n\nCycleGAN trains two generators and two discriminators, one pair per domain:\n\n- **Generators**\n  - \\(G: X \\rightarrow Y\\) maps photos \\(X\\) to Monet-style images \\(Y\\)\n  - \\(F: Y \\rightarrow X\\) maps Monet-style images \\(Y\\) back to photos \\(X\\)\n\n- **Discriminators**\n  - \\(D_Y\\) distinguishes real Monet images from generated Monet-style images \\(G(X)\\)\n  - \\(D_X\\) distinguishes real photos from generated photos \\(F(Y)\\)\n\n### 1) Adversarial Loss (BCE with logits)\n\nWe use a binary cross-entropy loss with logits (no sigmoid in the discriminator output). Each discriminator produces a PatchGAN logits map, and the loss is computed over all patches and averaged.\n\nFor a discriminator \\(D\\):\n- Real images should be classified as 1:\n  \\[\n  \\mathcal{L}_{D}^{real} = BCE(\\mathbf{1}, D(real))\n  \\]\n- Fake images should be classified as 0:\n  \\[\n  \\mathcal{L}_{D}^{fake} = BCE(\\mathbf{0}, D(fake))\n  \\]\n- Total discriminator loss:\n  \\[\n  \\mathcal{L}_{D} = \\frac{1}{2}\\left(\\mathcal{L}_{D}^{real} + \\mathcal{L}_{D}^{fake}\\right)\n  \\]\n\nFor a generator (adversarial term), fake images should be classified as real:\n\\[\n\\mathcal{L}_{GAN}(G, D_Y) = BCE(\\mathbf{1}, D_Y(G(X)))\n\\]\n\\[\n\\mathcal{L}_{GAN}(F, D_X) = BCE(\\mathbf{1}, D_X(F(Y)))\n\\]\n\n### 2) Cycle-Consistency Loss\n\nCycle-consistency encourages the translation to preserve the underlying content. After translating forward and back, we want to recover the original image:\n\n\\[\n\\mathcal{L}_{cyc}(G,F) =\n\\mathbb{E}_{x \\sim X}\\left[\\|F(G(x)) - x\\|_1\\right] +\n\\mathbb{E}_{y \\sim Y}\\left[\\|G(F(y)) - y\\|_1\\right]\n\\]\n\nWe use an \\(L1\\) reconstruction loss because it typically produces sharper reconstructions than \\(L2\\).","metadata":{}},{"cell_type":"code","source":"# =========================\n# CycleGAN - Training (CPU-only) + Load-or-Train + Persistent Save (Kaggle)\n# RULES (as requested):\n# - If saved generators exist -> LOAD them and SKIP training, SKIP saving.\n# - Else -> (optionally restore checkpoint) TRAIN, then SAVE generators.\n# =========================\n\nimport os\nimport time\nfrom pathlib import Path\nimport tensorflow as tf\n\n# -------------------------\n# Persistent paths (Kaggle)\n# -------------------------\nSAVE_DIR = Path(\"/kaggle/working/saved_cyclegan_models\")\nG_DIR = SAVE_DIR / \"G_photo2monet\"\nF_DIR = SAVE_DIR / \"F_monet2photo\"\nSAVE_DIR.mkdir(parents=True, exist_ok=True)\n\n# persistent checkpoints (optional resume)\nCKPT_DIR = \"/kaggle/working/checkpoints_cyclegan\"\nos.makedirs(CKPT_DIR, exist_ok=True)\n\n# -------------------------\n# Hyperparameters\n# -------------------------\nEPOCHS = 3\nSTEPS_PER_EPOCH = 25\nBATCH_SIZE = 4\nLR = 2e-4\nBETA_1 = 0.5\nLAMBDA_CYC = 10.0\n\n# -------------------------\n# Losses (BCE with logits + L1 cycle)\n# -------------------------\nbce_logits = tf.keras.losses.BinaryCrossentropy(from_logits=True)\nmae = tf.keras.losses.MeanAbsoluteError()\n\ndef discriminator_loss(d_real, d_fake):\n    real_loss = bce_logits(tf.ones_like(d_real), d_real)\n    fake_loss = bce_logits(tf.zeros_like(d_fake), d_fake)\n    return 0.5 * (real_loss + fake_loss)\n\ndef generator_gan_loss(d_fake):\n    return bce_logits(tf.ones_like(d_fake), d_fake)\n\ndef cycle_loss(real, cycled):\n    return mae(real, cycled)\n\n# -------------------------\n# Optimizers\n# -------------------------\ng_optimizer  = tf.keras.optimizers.Adam(learning_rate=LR, beta_1=BETA_1)\nf_optimizer  = tf.keras.optimizers.Adam(learning_rate=LR, beta_1=BETA_1)\ndx_optimizer = tf.keras.optimizers.Adam(learning_rate=LR, beta_1=BETA_1)\ndy_optimizer = tf.keras.optimizers.Adam(learning_rate=LR, beta_1=BETA_1)\n\n# -------------------------\n# Metrics (epoch averages)\n# -------------------------\nm_d_photo = tf.keras.metrics.Mean(name=\"D_photo_loss\")\nm_d_monet = tf.keras.metrics.Mean(name=\"D_monet_loss\")\nm_g_adv   = tf.keras.metrics.Mean(name=\"G_adv_loss\")\nm_f_adv   = tf.keras.metrics.Mean(name=\"F_adv_loss\")\nm_cyc_x   = tf.keras.metrics.Mean(name=\"cycle_X_loss\")\nm_cyc_y   = tf.keras.metrics.Mean(name=\"cycle_Y_loss\")\nm_g_total = tf.keras.metrics.Mean(name=\"G_total_loss\")\n\ndef reset_all_metrics():\n    for m in [m_d_photo, m_d_monet, m_g_adv, m_f_adv, m_cyc_x, m_cyc_y, m_g_total]:\n        m.reset_state()\n\n# -------------------------\n# Checkpointing (persistent)\n# -------------------------\nckpt = tf.train.Checkpoint(\n    G_photo2monet=G_photo2monet,\n    F_monet2photo=F_monet2photo,\n    D_photo=D_photo,\n    D_monet=D_monet,\n    g_optimizer=g_optimizer,\n    f_optimizer=f_optimizer,\n    dx_optimizer=dx_optimizer,\n    dy_optimizer=dy_optimizer,\n)\nckpt_manager = tf.train.CheckpointManager(ckpt, CKPT_DIR, max_to_keep=3)\n\n# -------------------------\n# SavedModel helpers (generators)\n# -------------------------\ndef saved_generators_exist():\n    # A SavedModel directory contains \"saved_model.pb\" (and variables/)\n    return (G_DIR / \"saved_model.pb\").exists() and (F_DIR / \"saved_model.pb\").exists()\n\ndef load_saved_generators():\n    print(\"âœ… Found saved generators. Loading and skipping training...\")\n    G_loaded = tf.keras.models.load_model(G_DIR, compile=False)\n    F_loaded = tf.keras.models.load_model(F_DIR, compile=False)\n    return G_loaded, F_loaded\n\ndef save_generators(G, F):\n    print(\"ðŸ’¾ Saving generators to:\", SAVE_DIR)\n    tf.keras.models.save_model(G, G_DIR, overwrite=True, include_optimizer=False)\n    tf.keras.models.save_model(F, F_DIR, overwrite=True, include_optimizer=False)\n    print(\"âœ… Saved generators:\", G_DIR, \"and\", F_DIR)\n\n# -------------------------\n# One training step\n# -------------------------\n@tf.function\ndef train_step(real_x, real_y):\n    with tf.GradientTape(persistent=True) as tape:\n        fake_y = G_photo2monet(real_x, training=True)    # X -> Y\n        fake_x = F_monet2photo(real_y, training=True)    # Y -> X\n\n        cycled_x = F_monet2photo(fake_y, training=True)  # X -> Y -> X\n        cycled_y = G_photo2monet(fake_x, training=True)  # Y -> X -> Y\n\n        d_real_x = D_photo(real_x, training=True)\n        d_fake_x = D_photo(fake_x, training=True)\n\n        d_real_y = D_monet(real_y, training=True)\n        d_fake_y = D_monet(fake_y, training=True)\n\n        dx_loss = discriminator_loss(d_real_x, d_fake_x)\n        dy_loss = discriminator_loss(d_real_y, d_fake_y)\n\n        g_adv = generator_gan_loss(d_fake_y)\n        f_adv = generator_gan_loss(d_fake_x)\n\n        cyc_x = cycle_loss(real_x, cycled_x)\n        cyc_y = cycle_loss(real_y, cycled_y)\n        g_total = g_adv + f_adv + (LAMBDA_CYC * (cyc_x + cyc_y))\n\n    g_grads  = tape.gradient(g_total, G_photo2monet.trainable_variables)\n    f_grads  = tape.gradient(g_total, F_monet2photo.trainable_variables)\n    dx_grads = tape.gradient(dx_loss,  D_photo.trainable_variables)\n    dy_grads = tape.gradient(dy_loss,  D_monet.trainable_variables)\n\n    g_optimizer.apply_gradients(zip(g_grads,  G_photo2monet.trainable_variables))\n    f_optimizer.apply_gradients(zip(f_grads,  F_monet2photo.trainable_variables))\n    dx_optimizer.apply_gradients(zip(dx_grads, D_photo.trainable_variables))\n    dy_optimizer.apply_gradients(zip(dy_grads, D_monet.trainable_variables))\n\n    m_d_photo.update_state(dx_loss)\n    m_d_monet.update_state(dy_loss)\n    m_g_adv.update_state(g_adv)\n    m_f_adv.update_state(f_adv)\n    m_cyc_x.update_state(cyc_x)\n    m_cyc_y.update_state(cyc_y)\n    m_g_total.update_state(g_total)\n\n# -------------------------\n# Training loop\n# -------------------------\ndef train(epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH):\n    photo_iter = iter(photo_ds)\n    monet_iter = iter(monet_ds)\n\n    for epoch in range(1, epochs + 1):\n        reset_all_metrics()\n        t0 = time.time()\n\n        for step in range(1, steps_per_epoch + 1):\n            real_x = next(photo_iter)\n            real_y = next(monet_iter)\n\n            train_step(real_x, real_y)\n\n            if step % 10 == 0 or step == 1:\n                print(\n                    f\"Epoch {epoch}/{epochs} | Step {step}/{steps_per_epoch} | \"\n                    f\"Dx={m_d_photo.result():.4f} Dy={m_d_monet.result():.4f} | \"\n                    f\"G_adv={m_g_adv.result():.4f} F_adv={m_f_adv.result():.4f} | \"\n                    f\"cycX={m_cyc_x.result():.4f} cycY={m_cyc_y.result():.4f} | \"\n                    f\"G_total={m_g_total.result():.4f}\"\n                )\n\n        ckpt_path = ckpt_manager.save(checkpoint_number=epoch)\n        dt = time.time() - t0\n        print(\n            f\"\\nâœ… Epoch {epoch} done in {dt/60:.2f} min | saved: {ckpt_path}\\n\"\n            f\"Final epoch metrics: \"\n            f\"Dx={m_d_photo.result():.4f}, Dy={m_d_monet.result():.4f}, \"\n            f\"G_adv={m_g_adv.result():.4f}, F_adv={m_f_adv.result():.4f}, \"\n            f\"cycX={m_cyc_x.result():.4f}, cycY={m_cyc_y.result():.4f}, \"\n            f\"G_total={m_g_total.result():.4f}\\n\"\n        )\n\n# =========================\n# MAIN: load-or-train\n# =========================\nif saved_generators_exist():\n    G_photo2monet, F_monet2photo = load_saved_generators()\nelse:\n    # optional resume from checkpoint\n    if ckpt_manager.latest_checkpoint:\n        ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()\n        print(\"âœ… Restored from checkpoint:\", ckpt_manager.latest_checkpoint)\n    else:\n        print(\"Initializing from scratch (no saved generators, no checkpoint).\")\n\n    train()\n    # Save ONLY because they did not exist\n    # save_generators(G_photo2monet, F_monet2photo)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T12:12:26.787528Z","iopub.execute_input":"2026-01-09T12:12:26.787879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# Visualization: Photo -> Monet (G) -> Photo (F)\n# Row 1: original photos (X)\n# Row 2: G_photo2monet(X)\n# Row 3: F_monet2photo(G_photo2monet(X))  (cycle back)\n# =========================\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\ndef denorm_m11_to_01(img):\n    # img in [-1, 1] -> [0, 1]\n    return tf.clip_by_value((img + 1.0) / 2.0, 0.0, 1.0)\n\n# Take one batch from photo_ds\nreal_x = next(iter(photo_ds))  # shape: (B, 256, 256, 3), in [-1, 1]\nreal_x_4 = real_x[:4]\n\n# Forward + cycle \nfake_y_4 = G_photo2monet(real_x_4, training=False)          # X -> Y\ncycled_x_4 = F_monet2photo(fake_y_4, training=False)        # X -> Y -> X\n\n# Convert to displayable [0,1]\nrow1 = denorm_m11_to_01(real_x_4)\nrow2 = denorm_m11_to_01(fake_y_4)\nrow3 = denorm_m11_to_01(cycled_x_4)\n\n# Plot 3x4\nplt.figure(figsize=(16, 10))\n\nfor i in range(4):\n    # Row 1: originals\n    ax = plt.subplot(3, 4, i + 1)\n    ax.imshow(row1[i].numpy())\n    ax.set_title(\"Original (Photo)\")\n    ax.axis(\"off\")\n\n    # Row 2: G output\n    ax = plt.subplot(3, 4, 4 + i + 1)\n    ax.imshow(row2[i].numpy())\n    ax.set_title(\"G: Photo â†’ Monet\")\n    ax.axis(\"off\")\n\n    # Row 3: F output on G output (cycled back)\n    ax = plt.subplot(3, 4, 8 + i + 1)\n    ax.imshow(row3[i].numpy())\n    ax.set_title(\"F: Monet â†’ Photo (Cycle)\")\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:30:57.808358Z","iopub.execute_input":"2026-01-09T16:30:57.808684Z","iopub.status.idle":"2026-01-09T16:31:00.223682Z","shell.execute_reply.started":"2026-01-09T16:30:57.808658Z","shell.execute_reply":"2026-01-09T16:31:00.221604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# Generate final Monet-style images with trained generator (CPU-only)\n# Produces N images as JPG (256x256) from PHOTO domain using G_photo2monet\n# =========================\n\nOUT_DIR = Path(\"./generated_monet_jpg\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nN_GEN = 7000\n\n# Ensure we use the trained generator in inference mode\nG_photo2monet.trainable = False\n\ndef m11_to_uint8(img_m11):\n    \"\"\"\n    img_m11: float tensor in [-1,1], shape (H,W,3)\n    returns uint8 array in [0,255]\n    \"\"\"\n    img01 = tf.clip_by_value((img_m11 + 1.0) / 2.0, 0.0, 1.0)\n    img255 = tf.cast(tf.round(img01 * 255.0), tf.uint8)\n    return img255.numpy()\n\n# Iterate over photos and generate\nphoto_iter = iter(photo_ds)\n\ncount = 0\nwhile count < N_GEN:\n    batch = next(photo_iter)  # (B,256,256,3) in [-1,1]\n    fake_y = G_photo2monet(batch, training=False)  # (B,256,256,3) in [-1,1]\n    if count % 10  == 0:\n        print(\"count: \", count)\n\n    b = fake_y.shape[0]\n    for i in range(b):\n        if count >= N_GEN:\n            break\n        img = m11_to_uint8(fake_y[i])\n        Image.fromarray(img).save(OUT_DIR / f\"monet_{count:05d}.jpg\", quality=95)\n        count += 1\n\nprint(f\"Saved {count} generated images to: {OUT_DIR.resolve()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:31:13.497552Z","iopub.execute_input":"2026-01-09T16:31:13.497924Z","iopub.status.idle":"2026-01-09T16:42:47.572997Z","shell.execute_reply.started":"2026-01-09T16:31:13.497892Z","shell.execute_reply":"2026-01-09T16:42:47.572034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\nfrom pathlib import Path\n\nzip_path = \"images.zip\"\nimg_dir = Path(\"./generated_monet_jpg\")\n\nwith zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n    for img_path in img_dir.glob(\"*.jpg\"):\n        zipf.write(img_path, arcname=img_path.name)\n\nprint(f\"Created submission file: {zip_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:46:40.920304Z","iopub.execute_input":"2026-01-09T16:46:40.920665Z","iopub.status.idle":"2026-01-09T16:46:48.528740Z","shell.execute_reply.started":"2026-01-09T16:46:40.920634Z","shell.execute_reply":"2026-01-09T16:46:48.527606Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Results\n\nThe trained CycleGAN is able to generate images that partially capture the visual style of Claude Monet.  \nThe generated images show noticeable changes in color distribution and texture compared to the original photographs, with softer edges and painterly color blending.\n\nDue to limited training time and computational constraints, the generated images do not fully match the complexity and richness of real Monet paintings. However, the outputs demonstrate that the model has learned meaningful stylistic features from the training data.\n","metadata":{}},{"cell_type":"markdown","source":"## Discussion\n\nThis project demonstrates the application of Generative Adversarial Networks to artistic style generation using an unpaired image dataset.  \nCycleGAN was selected because it enables image-to-image translation without paired examples, which aligns with the structure of the dataset.\n\nThe main limitations of this work include a small number of training epochs, reduced model capacity, and CPU-only training. These constraints were chosen intentionally to keep the project lightweight and focused on conceptual understanding rather than leaderboard optimization.\n","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\nIn this mini-project, a CycleGAN model was implemented and trained to generate Monet-style images from photographs.  \nThe project provided hands-on experience with GAN architectures, adversarial training, and unsupervised image translation.\n\nWhile the generated images are not of professional artistic quality, the results confirm that the model successfully learned stylistic patterns from the dataset. This work highlights both the creative potential of GANs and the importance of sufficient training and computational resources for high-quality image generation.\n","metadata":{}}]}